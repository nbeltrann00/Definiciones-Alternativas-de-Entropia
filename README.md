<p align="center">
  <img src="https://img.shields.io/badge/ENTROPÃA-TERMODINÃMICA%20%7C%20INFORMACIÃ“N%20%7C%20CUÃNTICA-blueviolet?style=for-the-badge&logo=python&logoColor=white" />
</p>

<h1 align="center">ğŸ“˜ Proyecto: ENTROPÃA</h1>

<p align="center">
  <b>DispersiÃ³n de la energÃ­a â€¢ Microestados â€¢ Shannon â€¢ Entrelazamiento â€¢ Dimensionalidad</b>
</p>

<hr>


# ğŸ“‘ Ãndice

1. [DefiniciÃ³n de la entropÃ­a como dispersiÃ³n de la energÃ­a](#definiciÃ³n-de-la-entropÃ­a-como-la-dispersiÃ³n-de-la-energÃ­a)
2. [Ejercicios prÃ¡cticos](#ejercicios-practicos)
3. [EntropÃ­a como distribuciÃ³n de microestados](#entropÃ­a-como-distribuciÃ³n-de-microestados)
    - [Microestados y Microestados](#microestados-y-microestados)
    - [La Segunda Ley, al Natural](#la-segunda-ley-al-natural)
4. [Ejemplos prÃ¡cticos](#ejemplos-prÃ¡cticos)
5. [EntropÃ­a como calidad de informaciÃ³n](#entropÃ­a-como-calidad-de-informaciÃ³n)
6. [EntropÃ­a como entrelazamiento de informaciÃ³n](#entropÃ­a-como-entrelazamiento-de-informaciÃ³n)
7. [EntropÃ­a como funciÃ³n de la dimensionalidad](#entropÃ­a-como-funciÃ³n-de-la-dimensionalidad)
8. [Segunda ley de la termodinÃ¡mica](#definiciÃ³n-y-ecuaciÃ³n-de-la-segunda-ley-de-la-termodinÃ¡mica)
9. [Ejemplo: CongelaciÃ³n del agua](#ejemplo-congelaciÃ³n-del-agua)
10. [Referencias](#referencias)


DefiniciÃ³n de la entropÃ­a como la dispersiÃ³n de la energÃ­a

Para tener una definiciÃ³n mÃ¡s moderna de la entropÃ­a, la definen Atkins y De Paula (2008) como la medida del grado de disipaciÃ³n de la energÃ­a en un sistema y alrededores,
En palabras textuales: 
â€œUna interpretaciÃ³n mÃ¡s satisfactoria de la entropÃ­a, es como medida de la disipaciÃ³n de la energÃ­a. Cuando ocurre un cambio, la energÃ­a se disipa mÃ¡s y la entropÃ­a aumenta. asÃ­ la segunda ley de la termodinÃ¡mica es, por lo tanto, la afirmaciÃ³n de que la energÃ­a tiene a disiparse y que existe una direcciÃ³n natural del cambio en la que la energÃ­a queda cada vez mÃ¡s disipada.â€ (Atkins y De Paula, 2008, pÃ¡g. 77-78)
TambiÃ©n incluyen el criterio universal del cambio espontaneo:
â€œHemos encontrado la seÃ±al indicadora del cambio espontaneo: buscamos la direcciÃ³n del cambio que conduce la disipaciÃ³n de energÃ­a total del sistema aisladoâ€ (Atkins & de Paula, 2008, p. 78)

Podemos definir la entropÃ­a S como la funciÃ³n de estado que cuantifica el grado en el que la energÃ­a de un proceso se ha disipado o distribuido entre todos los microestados accesibles del sistema y sus alrededores.
Un proceso espontÃ¡neo ocurre siempre en la direcciÃ³n en la que la energÃ­a total se disipa mÃ¡s, es decir, aquella en la que la variaciÃ³n de la entropÃ­a total del universo es mayor que cero:
Î”Sâ‚œâ‚’â‚œ = S sistema +S Alrededores> 0 
Resumiendo, asÃ­ la segunda ley como: 
Î”Stot > 0
(Atkins & de Paula, 2008, pp. 78â€“79)
EJERCICIOS PRACTICOS:
Enunciado Un cubito de hielo a 0 Â°C se pone en agua a 25 Â°C.
La energÃ­a tÃ©rmica del agua tibia se reparte hacia el hielo (lo derrite) y queda distribuida entre muchas mÃ¡s molÃ©culas en forma lÃ­quida. â†’ La energÃ­a total se disipa mÃ¡s que antes â†’ Î”S_tot > 0 â†’ proceso espontÃ¡neo. El proceso inverso (el agua lÃ­quida se congela sola sacando calor al ambiente mÃ¡s caliente) nunca ocurre porque disminuirÃ­a la disipaciÃ³n total de energÃ­a.
Enunciado Un recipiente dividido en dos partes: una con helio y otra con neÃ³n (misma T y P). Se quita la pared divisoria.
Las molÃ©culas de cada gas, que antes estaban confinadas en la mitad del volumen, ahora ocupan todo el recipiente. â†’ La energÃ­a cinÃ©tica de traslaciÃ³n se reparte en un volumen mayor nÃºmero de posiciones posibles â†’ la energÃ­a se disipa mÃ¡s â†’ Î”S_tot > 0 â†’ la mezcla es espontÃ¡nea. Nunca se ve que los gases se separen solos, porque eso concentrarÃ­a la energÃ­a en menos microestados.
EntropÃ­a como distribuciÃ³n de microestados

En la termodinÃ¡mica estadÃ­stica, la entropÃ­a S cuantifica la multiplicidad de microestados Î© accesibles a un macroestado especificado por variables extensivas como energÃ­a E, volumen V y nÃºmero de partÃ­culas N. Esta formulaciÃ³n, propuesta por Ludwig Boltzmann, fundamenta la irreversibilidad termodinÃ¡mica en el principio de mÃ¡xima probabilidad, donde los macroestados de alta entropÃ­a dominan debido a su superior degeneraciÃ³n microscÃ³pica (Reif, 1965).
Microestados y Microestados
Un microestado es como una "foto instantÃ¡nea completa" de todo lo que pasa dentro del sistema: la posiciÃ³n exacta (q) y la velocidad (p) de cada una de las N partÃ­culas. Imagina el "espacio de fase" como un mapa gigante en 6 dimensiones por partÃ­cula (3 para posiciÃ³n + 3 para velocidad), asÃ­ que para N partÃ­culas son 6N dimensiones en total.
El macroestado es una "regiÃ³n grande" en ese mapa (Î”Î“), que agrupa muchos microestados parecidos. Por ejemplo, todos los microestados donde el gas tiene energÃ­a E, volumen V y N molÃ©culas.
Entonces, Î© (el nÃºmero de microestados) se calcula como el tamaÃ±o de esa regiÃ³n dividido por h^{3N} N! (h es la constante de Planck para corregir unidades cuÃ¡nticas, y N! porque las partÃ­culas son idÃ©nticas y no distinguimos cuÃ¡l es cuÃ¡l).
Finalmente, la entropÃ­a sale como S = k_B ln Î©. Y cuando N es muy grande (lÃ­mite termodinÃ¡mico), esta S se comporta como una propiedad "normal" que suma si juntas sistemas (Reif, 1965).

La Segunda Ley, al Natural
La segunda ley dice que la entropÃ­a nunca baja (Î”S â‰¥ 0) en un sistema aislado. Â¿Por quÃ©? Porque Î© (nÃºmero de microestados) solo puede aumentar o quedarse igual. Las trayectorias en el espacio de fase conservan Î© (como billar), pero que el sistema "vaya hacia atrÃ¡s" a un Î© menor es como que todas las molÃ©culas se junten solas de nuevo: la probabilidad es e^{-N}, ridÃ­culamente pequeÃ±a cuando N es grande (tipo Avogadro).
Esto explica paradojas como el demonio de Maxwell: un ser que "mide" molÃ©culas para separar rÃ¡pidas de lentas. Parece violar la segunda ley, pero medir genera entropÃ­a extra (informaciÃ³n = correlaciÃ³n = desorden oculto), asÃ­ que al final todo suma (Reif, 1965).
Ejemplos prÃ¡cticos:
Ejemplo 1: EntropÃ­a de 20 monedas (Sistema simple de dos estados)
SituaciÃ³n: Imagina 20 monedas justas (cara o cruz). Calcula la entropÃ­a del macroestado con exactamente 10 caras y 10 cruces.
Paso 1: Identifica Î© (nÃºmero de microestados).
Es el nÃºmero de formas de elegir 10 caras de 20 monedas:
Î©=(20Â¦10)=20!/(10!â‹…10!)=184756
Paso 2: Calcula la entropÃ­a.
S=k_B lnâ¡Î©=k_B lnâ¡(184756)â‰ˆk_Bâ‹…12.43
(AquÃ­ k_B es la constante de Boltzmann; numÃ©ricamente ~1.38Ã—10^{-23} J/K, pero para sistemas macro se usa en unidades arbitrarias).
Paso 3: Compara con mÃ¡ximo.
MÃ¡xima Î© total = 2^{20} = 1,048,576 (todos los estados posibles).
S_mÃ¡x = k_B ln(2^{20}) = 20 k_B ln 2 â‰ˆ 13.86 k_B.
ConclusiÃ³n: Este macroestado tiene ~90% de la entropÃ­a mÃ¡xima, por eso es el mÃ¡s probable.
Ejemplo 2: Gas ideal expandiÃ©ndose (4 partÃ­culas en 2 compartimentos)
SituaciÃ³n: 4 partÃ­culas indistinguibles en un recipiente con 2 compartimentos iguales (izquierdo/derecho). Inicialmente todas a la izquierda; luego se libera la particiÃ³n.
Paso 1 (Inicial): Todas 4 a la izquierda.
Î©_inicial = 1 (solo una forma, partÃ­culas idÃ©nticas).
S_inicial = k_B ln 1 = 0.
Paso 2 (Final, equilibrio): PartÃ­culas distribuidas libremente (cada una tiene 2 choices).
Î©_final = 2^4 / 4! = 16 / 24 = 2/3? Espera, mejor: total microestados = \binom{4+2-1}{4} = 15 (distribuciÃ³n Bose para idÃ©nticas), pero simple: Î© = 2^4 = 16 sin dividir por N! para distinguishable approx.
(Aprox. distinguishable para claridad): Î©_final = 16.
S_final = k_B ln 16 â‰ˆ 2.77 k_B.
Paso 3: Cambio de entropÃ­a.
Î”S = S_final - S_inicial = 2.77 k_B > 0.
ConclusiÃ³n: La expansiÃ³n aumenta S porque hay 16 veces mÃ¡s formas de estar distribuidas que concentradas. Probabilidad de volver al inicial: 1/16 (Reif, 1965).
EntropÃ­a como calidad de informaciÃ³n 

La entropÃ­a de Shannon, desarrollada en 1948, redefine la entropÃ­a como una medida de incertidumbre o informaciÃ³n promedio en un mensaje o fuente de datos. No es "desorden fÃ­sico", sino cuÃ¡ntos bits necesitas en promedio para describir un evento aleatorio. Para una fuente con eventos de probabilidad p_i, la entropÃ­a mide la "sorpresa media" necesaria para codificar la informaciÃ³n (Shannon, 1948).
DefiniciÃ³n MatemÃ¡tica
Para una variable aleatoria discreta X con N posibles outcomes de probabilidades {p_1â“œ,p_2â“œ,...â“œ,p_N }:
H(X)=-âˆ‘_(i=1)^Nâ–’p_i  ã€–logâ¡ã€—_2 p_i
Unidades: bits (log base 2). Si usas ln, son nats.
Propiedades clave:
	H mÃ¡xima cuando todos p_i=1/N: H_max=ã€–logâ¡ã€—_2 N
	H=0 si un evento es cierto (p=1)
	Concava: mezcla probabilidades no reduce informaciÃ³n
ConexiÃ³n con la EntropÃ­a FÃ­sica
Edwin Jaynes (1957) demostrÃ³ que la fÃ³rmula de Boltzmann S=k_B lnâ¡Î© es idÃ©ntica a la entropÃ­a de Shannon cuando todos los microestados son equiprobables (p_i=1/Î©):
S=-k_Bâˆ‘p_i lnâ¡p_i=k_B lnâ¡Î©
La termodinÃ¡mica emerge como inferencia bayesiana mÃ¡xima de entropÃ­a bajo restricciones macroscÃ³picas conocidas (Jaynes, 1957).
Ejemplos prÃ¡cticos: 
Ejemplo 1: Cara/Cruz sesgada
Moneda con P(cara)=0.9, P(cruz)=0.1.
H=-[0.9ã€–logâ¡ã€—_2 0.9+0.1ã€–logâ¡ã€—_2 0.1]=-[0.9(-0.152)+0.1(-3.322)]=0.469" bits" 
ComparaciÃ³n: Moneda justa (0.5/0.5): H=1 bit.
InterpretaciÃ³n: La moneda sesgada da menos informaciÃ³n por tiro (mÃ¡s predecible), requiere menos bits para comprimir secuencias largas.
Ejemplo 2: Texto de inglÃ©s bÃ¡sico (CompresiÃ³n real)
Fuente: Texto simple con letras frecuentes en inglÃ©s bÃ¡sico:
e(0.30), t(0.20), a(0.15), o(0.10), i(0.08), n(0.07), r(0.05), s(0.05)
Paso 1: Construye el Ã¡rbol Huffman (proceso simplificado):
	Une los menos frecuentes: r+s = 0.10 â†’ nuevo nodo "rs"
	Une i+n = 0.15 â†’ nuevo nodo "in"
	Une "rs"(0.10) + o(0.10) = 0.20 â†’ nodo "rso"
	Ahora tienes: e(0.30), t(0.20), a(0.15), "in"(0.15), "rso"(0.20)
	Une a+"in" = 0.30 â†’ nodo "ain"
	Une t+"rso" = 0.40 â†’ nodo "trso"
	Final: e(0.30) vs "trso"(0.40) â†’ raÃ­z
Tabla de cÃ³digos resultantes:
Letra	Prob	CÃ³digo	Longitud
e	0.30	0	1
t	0.20	100	3
a	0.15	110	3
o	0.10	1010	4
i	0.08	1110	4
n	0.07	1111	4
r	0.05	10110	5
s	0.05	10111	5
Paso 2: Calcula tasa promedio de bits
L=0.30â‹…1+0.20â‹…3+0.15â‹…3+0.10â‹…4+0.08â‹…4+0.07â‹…4+0.05â‹…5+0.05â‹…5=3.05" bits/letra" 
Paso 3: EntropÃ­a teÃ³rica H
H=-âˆ‘p_i ã€–logâ¡ã€—_2 p_iâ‰ˆ2.95" bits/letra"
(CÃ³digos muy cercanos al Ã³ptimo)
ComparaciÃ³n prÃ¡ctica:
	CodificaciÃ³n fija (8 bits/letra, ASCII) = 8 bits
	Huffman = 3.05 bits â†’ 62% de compresiÃ³n
	Ejemplo texto: "the" â†’ t(100)+h(?)+e(0) = ~7 bits vs 24 bits fijos
ConclusiÃ³n: En textos reales, Huffman ahorra ~60% espacio. ZIP/JPG lo usan combinado con otros mÃ©todos (Shannon, 1948).

EntropÃ­a como entrelazamiento de informaciÃ³n 
IntroducciÃ³n
En mecÃ¡nica cuÃ¡ntica, la entropÃ­a de von Neumann generaliza la entropÃ­a de Shannon al mundo cuÃ¡ntico, midiendo la "mezcla" o pureza de un estado cuÃ¡ntico. Para sistemas entrelazados, captura la correlaciÃ³n cuÃ¡ntica no clÃ¡sica que no se puede describir por estados separados. Cuando dos partÃ­culas estÃ¡n entrelazadas, su entropÃ­a conjunta es baja (estado puro), pero cada subsistema por separado tiene entropÃ­a mÃ¡xima (totalmente mezclada), cuantificando asÃ­ el grado de entrelazamiento (Nielsen & Chuang, 2000).
DefiniciÃ³n MatemÃ¡tica
Para un sistema cuÃ¡ntico con matriz densidad Ï:
S(Ï)=-"Tr"(Ïã€–logâ¡ã€—_2 Ï)
Propiedades clave:
	Estado puro: Ï=âˆ£ÏˆâŸ©âŸ¨Ïˆâˆ£, eigenvalues=1,0 â†’ S=0
	Estado mÃ¡ximo mezclado (qubit): Ï=I/2, S=1 bit
	Subaditividad: S(AB)â‰¤S(A)+S(B), igualdad si no entrelazados
Diferencia con Shannon
Shannon asume observables conmutativos (clÃ¡sicos). Von Neumann maneja no-conmutatividad:
S(Ï_AB)+S(Ï_A)+S(Ï_B)â‰¥S(Ï_AâŠ—Ï_B)
La discordancia cuÃ¡ntica mide exceso de correlaciÃ³n cuÃ¡ntica sobre clÃ¡sica.
Ejemplo 1: Par de Bell (MÃ¡ximo entrelazamiento)
Estado: âˆ£Î¦^+âŸ©=(âˆ£00âŸ©+âˆ£11âŸ©)/âˆš2
Paso 1: Sistema total (puro): Ï_AB=âˆ£Î¦^+âŸ©âŸ¨Î¦^+âˆ£, S(Ï_AB)=0
Paso 2: Subistema A (traza parcial): Ï_A=ã€–"Tr" ã€—_B (Ï_AB)=I/2, eigenvalues=[0.5,0.5]
S(Ï_A)=-2â‹…(0.5ã€–logâ¡ã€—_2 0.5)=1" bit"
Igual para Ï_B.
InterpretaciÃ³n: Sistema total "sabe todo" (S=0), pero cada mitad estÃ¡ completamente incierta (S=1). El entrelazamiento es la "informaciÃ³n compartida cuÃ¡nticamente".
Ejemplo 2: Dos qubits con magnetizaciÃ³n parcial
Estado: âˆš0.8âˆ£00âŸ©+âˆš0.2âˆ£11âŸ©
Tabla de entropÃ­as:
Estado	S(Ï_AB)	S(Ï_A)	S(Ï_B)	Entrelazamiento
Bell	0	1	1	MÃ¡ximo (1 bit)
Separado	0.32	0.50	0.50	Bajo (0.18 bit)
Producto	0.32	0.11	0.32	Ninguno (0)
CÃ¡lculo detallado: Ï_A=(â– (0.8&0@0&0.2))
S(Ï_A)=-[0.8ã€–logâ¡ã€—_2 0.8+0.2ã€–logâ¡ã€—_2 0.2]â‰ˆ0.50" bits"
Entrelazamiento cuantificado: E=S(Ï_A)=0.50 bits (para estados puros).
Aplicaciones PrÃ¡cticas
	CriptografÃ­a cuÃ¡ntica: Protocolo BB84 usa entropÃ­a para detectar espionaje (Eve introduce entropÃ­a).
	ComputaciÃ³n cuÃ¡ntica: Estados con S(Ï_A)â‰ˆ1 para cada qubit son ideales para qubits lÃ³gicos.
	TeletransportaciÃ³n: Requiere entropÃ­a mÃ¡xima en pares EPR precompartidos.
ConexiÃ³n con TermodinÃ¡mica
La entropÃ­a de von Neumann reduce a Shannon para bases comunes, y para muchos partÃ­culas no interactuantes reproduce Boltzmann. El principio de mÃ¡xima entropÃ­a cuÃ¡ntico de Jaynes da distribuciones tÃ©rmicas desde restricciones de energÃ­a (Vedral et al., 1997).
EntropÃ­a como funciÃ³n de la dimensionalidad 

IntroducciÃ³n
En teorÃ­a de la informaciÃ³n y aprendizaje automÃ¡tico, la entropÃ­a crece explosivamente con la dimensionalidad D del espacio de datos ("maldiciÃ³n de la dimensionalidad"). En Dâ†’âˆ, el volumen se concentra en la cÃ¡scara superficial, haciendo HâˆDÃ—2á´°. El principio hologrÃ¡fico resuelve esto: toda informaciÃ³n D-dimensional se codifica eficientemente en (D-1)-dimensional, como en AdS/CFT donde gravedad 5D=teorÃ­a de campos 4D en frontera (Bellman, 1961; Maldacena, 1998).
FÃ³rmula clave: EntropÃ­a volumÃ©trica
Datos uniformes en hipercubo ^D:
H_D=Dã€–logâ¡ã€—_2 L+ã€–logâ¡ã€—_2 ("volumen efectivo")
Volumen accesible: ~99.999% en superficie R^{D-1} para D>10.
EntropÃ­a superficial: SâˆDâ‹…R^(D-1) (Â¡como agujeros negros!).
Ejemplo 1: Dado en D dimensiones
DimensiÃ³n	Estados	EntropÃ­a (bits)	% en superficie
1D	6	2.58	100%
3D	216	7.76	85%
10D	60M	25.9	99.999%
100D	6^100	259	~100%
InterpretaciÃ³n: En alta D, puntos "viven" en la cÃ¡scara. H explota exponencialmente (Bellman, 1961).
Ejemplo 2: MNIST (784 dims â†’ manifold real)
	TeÃ³rica: 784Ã—8=6272 bits
	Efectiva: ~120 bits (datos en manifold ~10D)
	PCA: 154 dims (95% varianza) â†’ Hâ‰ˆ154 bits
	Autoencoder: 32 dims â†’ Hâ‰ˆ32 bits (holografÃ­a computacional)
RelaciÃ³n con Principio HologrÃ¡fico
Paralelismo perfecto:
MaldiciÃ³n Dimensionalidad	Principio HologrÃ¡fico
H âˆ D Ã— 2á´° (volumen)	S âˆ RÂ² (Ã¡rea)
Datos diluidos en Dâ†‘	Info codificada en D-1
Sampling imposible (D>50)	LÃ­mite: 1 bit/4lâ‚šÂ²
ML: PCA, autoencoders	FÃ­sica: AdS/CFT
ConexiÃ³n matemÃ¡tica: En espacios de alta D, volumen efectivo â†’ superficie:
ã€–limâ¡ã€—_(Dâ†’âˆ)  V_"interior" /V_"total"  â†’0
H_"efectiva" â‰ˆH_"superficie" âˆ(D-1)logâ¡R (Maldacena, 1998).
AdS/CFT como "holografÃ­a de datos":
	Bulk (5D gravedad): Datos "volumÃ©tricos"
	Boundary (4D CFT): CompresiÃ³n lossless en frontera
	EntropÃ­a igual: S_"bulk" =S_"boundary"  ('t Hooft, 1993).
Ejemplo 3: HolografÃ­a en ML prÃ¡ctica
python
# Imagen 512Ã—512 = 262k dims â†’ bottleneck 512 dims
entropia_raw = 2M bits      # Pixel-wise
entropia_holo = 512 bits    # Manifold superficie
compresion = 99.97%         # Como S= A/4lâ‚šÂ²
Implicaciones unificadas
	Universo computacional: Â¿Nuestro 3D emerge de 2D informacional?
	LÃ­mite fÃ­sico-ML: Autoencoders = duality hologrÃ¡fica
	Big Data: Nunca proceses D>1000 sin reducciÃ³n dimensional (Bellman, 1961).
ConclusiÃ³n: La maldiciÃ³n dimensionalidad + holografÃ­a = teorema universal: toda informaciÃ³n compleja vive en su "frontera", sea fÃ­sica o datos.





â€ƒ
DefiniciÃ³n y ecuaciÃ³n de la segunda ley de la termodinÃ¡mica
La segunda ley de la termodinÃ¡mica se basa en definir la direcciÃ³n de los procesos naturales en un sistema aislado aumenta espontÃ¡neamente. Esto se representa como entropÃ­a total o entropÃ­a del universo [Î”Suniv]. A su vez esta ley complementa a la primera ley de la termodinÃ¡mica la cual establece que la energÃ­a se conserva, pero no predice la direcciÃ³n de cambio.
EntropÃ­a (S) y Cambio de EntropÃ­a (Î”S):
A partir del libro Chemistry (Flowers, Theopold, Langley Richard, & Robinson, 2015, pÃ¡gs. 891-893) podemos tomar entropÃ­a(S) como desorden o aleatoriedad de un sistema en un estado especifico. Ahora bien, el cambio de la entropÃ­a lo podemos calcular con: 
Î”S = S_final â€“ S_inicial
Otra cosa para tener en cuenta es que Î”S_univ se puede calcular de la forma:
Î”S_univ = Î”S_sis + Î”S_alr
Siendo Î”S_sis el cambio de entropÃ­a del sistema y Î”S_alr el cambio de entropÃ­a de los alrededores. En caso de necesitar calcular Î”S_alr podemos tomar la formula Î”S_alr = (-Î”H_sis) / T. 
Como un extra podrÃ­amos tener en cuenta la energÃ­a libre de Gibbs (Î”G = Î”H - T * Î”S_sis) pero para este repositorio no profundizaremos sobre esta misma.
Partiendo es estos cÃ¡lculos podrÃ­amos tomar criterios claves para determinar si el proceso es espontaneo o no espontaneo. Los criterios para esto son:
Si Î”S_univ > 0, el proceso es EspontÃ¡neo.
Si Î”S_univ < 0, el proceso es No espontÃ¡neo.
Si Î”S_univ = 0, el proceso es Reversible
Ejemplos:
Â¿Se derretirÃ¡ el hielo espontÃ¡neamente?
La variaciÃ³n de entropÃ­a del proceso Hâ‚‚O(s) âŸ¶ Hâ‚‚O(l) es de 22,1 J/K y requiere que el entorno transfiera 6,00 kJ de calor al sistema. Â¿Es el proceso espontÃ¡neo a âˆ’10,00 Â°C? Â¿Es espontÃ¡neo a +10,00 Â°C?
ParÃ¡metro	CÃ¡lculo	Resultado
Î”S_alr	(-Î”H_sis) / T = -(-6000 J) / 263.15 K	+22.80 J/K
Î”S_univ	Î”S_sis + Î”S_alr = -22.1 J/K + 22.80 J/K	+0.70 J/K

RTA: La congelaciÃ³n es espontÃ¡nea a -10 Â°C al Î”S_univ ser positivo
ParÃ¡metro	CÃ¡lculo	Resultado
Î”S_alr	(-Î”H_sis) / T = -(-6000 J) / 273.16 K	+21.97 J/K
Î”S_univ	Î”S_sis + Î”S_alr = -22.1 J/K + 21.97 J/K	-0.13 J/K

La congelaciÃ³n es no espontÃ¡nea a 10 Â°C al Î”S_univ ser negativo
Referencias
Referencias Completas en Formato APA (7ma ediciÃ³n)
FÃ­sica EstadÃ­stica (Boltzmann/Microestados)
Reif, F. (1965). Fundamentals of statistical and thermal physics. McGraw-Hill.
TeorÃ­a de la InformaciÃ³n
Shannon, C. E. (1948). A mathematical theory of communication. The Bell System Technical Journal, 27(3), 379-423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x
InformaciÃ³n CuÃ¡ntica
Nielsen, M. A., & Chuang, I. L. (2000). Quantum computation and quantum information. Cambridge University Press. https://doi.org/10.1017/CBO9780511976667
Vedral, V., Plenio, M. B., Rippin, M. A., & Knight, P. L. (1997). Rigorous conditions for pure-state convertibility and their application to entanglement. Physical Review Letters, 78(17), 3217-3220. https://doi.org/10.1103/PhysRevLett.78.3217
Principio HologrÃ¡fico
't Hooft, G. (1993). Dimensional reduction in quantum gravity. arXiv:gr-qc/9310026. https://arxiv.org/abs/gr-qc/9310026
Susskind, L. (1995). The world as a hologram. Journal of Mathematical Physics, 36(11), 6377-6396. https://doi.org/10.1063/1.531249
Maldacena, J. (1998). The large N limit of superconformal field theories and supergravity. Advances in Theoretical and Mathematical Physics, 2, 231-252. https://doi.org/10.4310/ATMP.1998.v2.n2.a1
Dimensionalidad y Machine Learning
Bellman, R. (1961). Adaptive control processes: A guided tour. Princeton University Press.
Jaynes (Puente termodinÃ¡mica-informaciÃ³n)
Jaynes, E. T. (1957). Information theory and statistical mechanics. Physical Review, 106(4), 620-630. https://doi.org/10.1103/PhysRev.106.620
Jaynes, E. T. (1957). Information theory and statistical mechanics II. Physical Review, 108(2), 171-190. https://doi.org/10.1103/PhysRev.108.171
Flowers, P., Theopold, K., Langley Richard, & Robinson, W.R. (2015). Chemistry. Houston: OpenStax.
