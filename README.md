ğŸ“‹ Ãndice de ContenidosEntropÃ­a como DispersiÃ³n de EnergÃ­aEjercicios PrÃ¡cticos (EntropÃ­a ClÃ¡sica)EntropÃ­a como DistribuciÃ³n de Microestados (Boltzmann)EntropÃ­a como Calidad de InformaciÃ³n (Shannon)EntropÃ­a como Entrelazamiento de InformaciÃ³n (Von Neumann)EntropÃ­a como FunciÃ³n de la Dimensionalidad (HolografÃ­a)EcuaciÃ³n de la Segunda Ley de la TermodinÃ¡mica y CriteriosEjemplos Detallados de EspontaneidadReferencias1. EntropÃ­a como DispersiÃ³n de EnergÃ­aLa entropÃ­a se define como la dispersiÃ³n de la energÃ­a1. Para una definiciÃ³n mÃ¡s moderna, Atkins y De Paula (2008) la definen como la medida del grado de disipaciÃ³n de la energÃ­a en un sistema y alrededores2.En palabras textuales:â€œUna interpretaciÃ³n mÃ¡s satisfactoria de la entropÃ­a, es como medida de la disipaciÃ³n de la energÃ­a. Cuando ocurre un cambio, la energÃ­a se disipa mÃ¡s y la entropÃ­a aumenta. AsÃ­, la segunda ley de la termodinÃ¡mica es, por lo tanto, la afirmaciÃ³n de que la energÃ­a tiene a disiparse y que existe una direcciÃ³n natural del cambio en la que la energÃ­a queda cada vez mÃ¡s disipada.â€ 3TambiÃ©n incluyen el criterio universal del cambio espontÃ¡neo:â€œHemos encontrado la seÃ±al indicadora del cambio espontÃ¡neo: buscamos la direcciÃ³n del cambio que conduce la disipaciÃ³n de energÃ­a total del sistema aisladoâ€4.Podemos definir la entropÃ­a S como la funciÃ³n de estado que cuantifica el grado en el que la energÃ­a de un proceso se ha disipado o distribuido entre todos los microestados accesibles del sistema y sus alrededores5.Un proceso espontÃ¡neo ocurre siempre en la direcciÃ³n en la que la energÃ­a total se disipa mÃ¡s 6, es decir, aquella en la que la variaciÃ³n de la entropÃ­a total del universo es mayor que cero7:$$\Delta S_{\text{tot}} = S_{\text{sistema}} + S_{\text{alrededores}} > 0$$8Resumiendo, la Segunda Ley queda como:$$\Delta S_{\text{tot}} > 0$$92. Ejercicios PrÃ¡cticos (EntropÃ­a ClÃ¡sica)Enunciado 1: Hielo en aguaUn cubito de hielo a $0 \, \text{Â°C}$ se pone en agua a $25 \, \text{Â°C}$10.La energÃ­a tÃ©rmica del agua tibia se reparte hacia el hielo (lo derrite) y queda distribuida entre muchas mÃ¡s molÃ©culas en forma lÃ­quida11.ConclusiÃ³n: La energÃ­a total se disipa mÃ¡s que antes. $\implies \Delta S_{\text{tot}} > 0 \implies$ Proceso espontÃ¡neo12.El proceso inverso (el agua lÃ­quida se congela sola sacando calor al ambiente mÃ¡s caliente) nunca ocurre porque disminuirÃ­a la disipaciÃ³n total de energÃ­a13.Enunciado 2: Mezcla de GasesUn recipiente dividido en dos partes: una con helio y otra con neÃ³n (misma T y P). Se quita la pared divisoria14.Las molÃ©culas de cada gas, que antes estaban confinadas en la mitad del volumen, ahora ocupan todo el recipiente15.ConclusiÃ³n: La energÃ­a cinÃ©tica de traslaciÃ³n se reparte en un volumen mayor nÃºmero de posiciones posibles $\implies$ la energÃ­a se disipa mÃ¡s $\implies \Delta S_{\text{tot}} > 0 \implies$ La mezcla es espontÃ¡nea16.Nunca se ve que los gases se separen solos, porque eso concentrarÃ­a la energÃ­a en menos microestados17.3. EntropÃ­a como DistribuciÃ³n de Microestados (Boltzmann)En la termodinÃ¡mica estadÃ­stica, la entropÃ­a S cuantifica la multiplicidad de microestados Î© accesibles a un macroestado especificado por variables extensivas como energÃ­a $\text{E}$, volumen $\text{V}$ y nÃºmero de partÃ­culas $\text{N}$18.Esta formulaciÃ³n (Ludwig Boltzmann) fundamenta la irreversibilidad termodinÃ¡mica en el principio de mÃ¡xima probabilidad, donde los macroestados de alta entropÃ­a dominan debido a su superior degeneraciÃ³n microscÃ³pica19.Microestados y MacroestadosMicroestado: Es una "foto instantÃ¡nea completa" de la posiciÃ³n exacta $(\text{q})$ y la velocidad $(\text{p})$ de cada una de las $\text{N}$ partÃ­culas20.Macroestado: Es una "regiÃ³n grande" que agrupa muchos microestados parecidos (ej. todos los microestados donde el gas tiene energÃ­a $\text{E}$, volumen $\text{V}$ y $\text{N}$ molÃ©culas)21.La entropÃ­a se relaciona con el nÃºmero de microestados accesibles $\Omega$ mediante la ecuaciÃ³n de Boltzmann22:$$S = k_{B} \ln \Omega$$La Segunda Ley, al NaturalLa segunda ley dice que la entropÃ­a nunca baja ($\Delta S \ge 0$) en un sistema aislado23. Esto se debe a que $\Omega$ (nÃºmero de microestados) solo puede aumentar o quedarse igual24.Ejemplo 1: EntropÃ­a de 20 monedasSituaciÃ³n: Calcula la entropÃ­a del macroestado con exactamente 10 caras y 10 cruces en 20 monedas justas25.Identifica Î© (nÃºmero de microestados):$$\Omega = \binom{20}{10} = \frac{20!}{10! [cite_start]\cdot 10!} = 184.756$$26Calcula la entropÃ­a:$$S = k_B \ln \Omega \approx k_B \cdot 12.43$$27ConclusiÃ³n: Este macroestado tiene aproximadamente el 90% de la entropÃ­a mÃ¡xima, por eso es el mÃ¡s probable28.Ejemplo 2: Gas ideal expandiÃ©ndose (4 partÃ­culas en 2 compartimentos)SituaciÃ³n: 4 partÃ­culas indistinguibles en un recipiente con 2 compartimentos. Inicialmente todas a la izquierda; luego se libera la particiÃ³n29.Paso Inicial (Concentrado): $\Omega_{\text{inicial}} = 1$. $\implies S_{\text{inicial}} = k_B \ln 1 = 0$30.Paso Final (Equilibrio): $\Omega_{\text{final}} \approx 16$ (si son partÃ­culas distinguibles para claridad). $\implies S_{\text{final}} = k_B \ln 16 \approx 2.77 k_B$31.Cambio de entropÃ­a: $\Delta S = S_{\text{final}} - S_{\text{inicial}} = 2.77 k_B > 0$32.ConclusiÃ³n: La expansiÃ³n aumenta S porque hay 16 veces mÃ¡s formas de estar distribuidas que concentradas33.4. EntropÃ­a como Calidad de InformaciÃ³n (Shannon)La entropÃ­a de Shannon (1948) redefine la entropÃ­a como una medida de incertidumbre o informaciÃ³n promedio en un mensaje o fuente de datos34. No es "desorden fÃ­sico", sino cuÃ¡ntos bits necesitas en promedio para describir un evento aleatorio35.DefiniciÃ³n MatemÃ¡ticaPara una variable aleatoria discreta $\text{X}$ con $\text{N}$ posibles resultados de probabilidades $p_i$:$$H(X) = - \sum_{i=1}^{N} p_i \log_2 p_i$$36Las unidades son bits (si se usa $\log_2$)37.ConexiÃ³n con la EntropÃ­a FÃ­sicaEdwin Jaynes (1957) demostrÃ³ que la fÃ³rmula de Boltzmann $S = k_{B} \ln \Omega$ es idÃ©ntica a la entropÃ­a de Shannon cuando todos los microestados son equiprobables38.La termodinÃ¡mica emerge como inferencia bayesiana de mÃ¡xima entropÃ­a bajo restricciones macroscÃ³picas conocidas39.Ejemplo 1: Cara/Cruz sesgadaMoneda con $P(\text{cara}) = 0.9$, $P(\text{cruz}) = 0.1$40.$$H = - [0.9 \log_2 0.9 + 0.1 \log_2 0.1] \approx 0.469 \text{ bits}$$41ComparaciÃ³n: Moneda justa ($0.5/0.5$): $H = 1 \text{ bit}$42.InterpretaciÃ³n: La moneda sesgada da menos informaciÃ³n por tiro (es mÃ¡s predecible)43.Ejemplo 2: Texto de inglÃ©s bÃ¡sico (CompresiÃ³n real)Se calcula la tasa promedio de bits $\text{L}$ usando cÃ³digos Huffman, obteniendo44:$$L \approx 3.05 \text{ bits/letra}$$EntropÃ­a TeÃ³rica (Shannon): $H \approx 2.95 \text{ bits/letra}$ 45 (Los cÃ³digos son muy cercanos al Ã³ptimo teÃ³rico).ConclusiÃ³n: La codificaciÃ³n Huffman logra un $\approx 62\%$ de compresiÃ³n comparado con una codificaciÃ³n fija de 8 bits/letra (ASCII)46.5. EntropÃ­a como Entrelazamiento de InformaciÃ³n (Von Neumann)En mecÃ¡nica cuÃ¡ntica, la entropÃ­a de von Neumann generaliza la entropÃ­a de Shannon al mundo cuÃ¡ntico, midiendo la "mezcla" o pureza de un estado cuÃ¡ntico47. Para sistemas entrelazados, captura la correlaciÃ³n cuÃ¡ntica no clÃ¡sica48.Cuando dos partÃ­culas estÃ¡n entrelazadas, su entropÃ­a conjunta es baja (estado puro), pero cada subsistema por separado tiene entropÃ­a mÃ¡xima (totalmente mezclada), cuantificando asÃ­ el grado de entrelazamiento49.DefiniciÃ³n MatemÃ¡ticaPara un sistema cuÃ¡ntico con matriz densidad $\rho$:$$S(\rho) = - \text{Tr} (\rho \log_2 \rho)$$50Ejemplo 1: Par de Bell (MÃ¡ximo entrelazamiento)Estado: $|\Phi^{+}\rangle = \frac{|00\rangle + |11\rangle}{\sqrt{2}}$51.Sistema Total (puro): $S(\rho_{AB}) = 0$52.Subsistema A (mezclado): $S(\rho_A) = 1 \text{ bit}$53.InterpretaciÃ³n: El sistema total "sabe todo" ($S=0$), pero cada mitad estÃ¡ completamente incierta ($S=1$)54. El entrelazamiento es la "informaciÃ³n compartida cuÃ¡nticamente"55.6. EntropÃ­a como FunciÃ³n de la Dimensionalidad (HolografÃ­a)En teorÃ­a de la informaciÃ³n y aprendizaje automÃ¡tico, la entropÃ­a crece explosivamente con la dimensionalidad $\text{D}$ del espacio de datos ("maldiciÃ³n de la dimensionalidad")56.El principio hologrÃ¡fico resuelve esto: toda informaciÃ³n D-dimensional se codifica eficientemente en (D-1)-dimensional57.FÃ³rmula Clave: EntropÃ­a volumÃ©tricaPara datos uniformes en hipercubo $\text{D}$:$$H_D \propto D \log_2 L + \log_2 (\text{volumen efectivo})$$58RelaciÃ³n con Principio HologrÃ¡ficoExiste un paralelismo perfecto entre la maldiciÃ³n de la dimensionalidad y el principio hologrÃ¡fico59:| MaldiciÃ³n Dimensionalidad | Principio HologrÃ¡fico || :--- | :--- || $\text{H} \propto \text{D} \times 2^{\text{D}}$ (volumen) 60 | $\text{S} \propto \text{R}^2$ (Ã¡rea) 6161 || Datos diluidos en $\text{D} \uparrow$ 62 | Info codificada en $\text{D}-1$ 63 || ML: PCA, autoencoders 64 | FÃ­sica: AdS/CFT 65 |ConexiÃ³n con TermodinÃ¡mica: La entropÃ­a superficial ($S \propto R^2$) se comporta como la entropÃ­a de los agujeros negros66.7. EcuaciÃ³n de la Segunda Ley de la TermodinÃ¡mica y CriteriosLa Segunda Ley de la TermodinÃ¡mica se basa en definir la direcciÃ³n de los procesos naturales en un sistema aislado: la entropÃ­a aumenta espontÃ¡neamente67. Esto se representa como entropÃ­a total o entropÃ­a del universo (Î”S_univ)68. A su vez, esta ley complementa a la Primera Ley de la TermodinÃ¡mica (conservaciÃ³n de la energÃ­a), la cual no predice la direcciÃ³n de cambio69.EntropÃ­a (S) y Cambio de EntropÃ­a (Î”S)EntropÃ­a (S): Desorden o aleatoriedad de un sistema en un estado especÃ­fico70.Cambio de EntropÃ­a (Î”S): Se calcula con71:$$\Delta S = S_{\text{final}} - S_{\text{inicial}}$$72FÃ³rmulas ClaveEntropÃ­a del Universo:$$\Delta S_{\text{univ}} = \Delta S_{\text{sis}} + \Delta S_{\text{alr}}$$73Î”S de los Alrededores:$$\Delta S_{\text{alr}} = \frac{-\Delta H_{\text{sis}}}{T}$$74EnergÃ­a Libre de Gibbs (Mencionado como extra):$$\Delta G = \Delta H - T \cdot \Delta S_{\text{sis}}$$75Criterios de EspontaneidadCondiciÃ³nProcesoÎ”S_univ > 0 76EspontÃ¡neo 77Î”S_univ < 0 78No espontÃ¡neo 79Î”S_univ = 0 80Reversible (Equilibrio) 818. Ejemplos Detallados de EspontaneidadEnunciado: Â¿Se derretirÃ¡ el hielo espontÃ¡neamente?La variaciÃ³n de entropÃ­a del proceso $\text{H}_2\text{O}(s) \to \text{H}_2\text{O}(l)$ es de $22.1 \, \text{J/K}$ y requiere que el entorno transfiera $6.00 \, \text{kJ}$ de calor al sistema82. (Nota: Los cÃ¡lculos a continuaciÃ³n se basan en el proceso de congelaciÃ³n, el proceso inverso).Datos (para FusiÃ³n): $\Delta S_{\text{sis}} = +22.1 \, \text{J/K}$ y $q_{\text{sis}} = +6000 \, \text{J}$.Datos (para CongelaciÃ³n): $\Delta S_{\text{sis}} = -22.1 \, \text{J/K}$ y $\Delta H_{\text{sis}} = -6000 \, \text{J}$.A. CÃ¡lculo a $-10.00 \, \text{Â°C}$ ($263.15 \, \text{K}$)ParÃ¡metroCÃ¡lculoResultadoÎ”S_alr$\frac{-\Delta H_{\text{sis}}}{T} = \frac{-(-6000 \, \text{J})}{263.15 \, \text{K}}$ 83$+22.80 \, \text{J/K}$ 84Î”S_univ$\Delta S_{\text{sis}} + \Delta S_{\text{alr}} = -22.1 \, \text{J/K} + 22.80 \, \text{J/K}$ 85$+0.70 \, \text{J/K}$ 86Respuesta: La congelaciÃ³n es espontÃ¡nea a $-10 \, \text{Â°C}$ al $\Delta S_{\text{univ}}$ ser positivo87.B. CÃ¡lculo a $+10.00 \, \text{Â°C}$ ($283.15 \, \text{K}$)(El archivo de Word utiliza $273.16 \, \text{K}$ en el cÃ¡lculo que corresponde a $0.01 \, \text{Â°C}$, pero la conclusiÃ³n aplica para temperaturas por encima del punto de congelaciÃ³n).ParÃ¡metroCÃ¡lculoResultadoÎ”S_alr$\frac{-\Delta H_{\text{sis}}}{T} = \frac{-(-6000 \, \text{J})}{273.16 \, \text{K}}$ 88$+21.97 \, \text{J/K}$ 89Î”S_univ$\Delta S_{\text{sis}} + \Delta S_{\text{alr}} = -22.1 \, \text{J/K} + 21.97 \, \text{J/K}$ 90$-0.13 \, \text{J/K}$ 91Respuesta: La congelaciÃ³n es no espontÃ¡nea a $10 \, \text{Â°C}$ al $\Delta S_{\text{univ}}$ ser negativo92.9. ReferenciasSe presentan las referencias completas en formato APA (7ma ediciÃ³n)93.FÃ­sica EstadÃ­stica (Boltzmann/Microestados)Reif, F. (1965). Fundamentals of statistical and thermal physics. McGraw-Hill94.TeorÃ­a de la InformaciÃ³nShannon, C. E. (1948). A mathematical theory of communication. The Bell System Technical Journal, 27(3), 379-42395.Jaynes, E. T. (1957). Information theory and statistical mechanics. Physical Review, 106(4), 620-63096.Jaynes, E. T. (1957). Information theory and statistical mechanics II. Physical Review, 108(2), 171-19097.InformaciÃ³n CuÃ¡nticaNielsen, M. A., & Chuang, I. L. (2000). Quantum computation and quantum information. Cambridge University Press98.Vedral, V., Plenio, M. B., Rippin, M. A., & Knight, P. L. (1997). Rigorous conditions for pure-state convertibility and their application to entanglement. Physical Review Letters, 78(17), 3217-322099.Principio HologrÃ¡fico't Hooft, G. (1993). Dimensional reduction in quantum gravity. arXiv:gr-qc/9310026100.Susskind, L. (1995). The world as a hologram. Journal of Mathematical Physics, 36(11), 6377-6396101.Maldacena, J. (1998). The large N limit of superconformal field theories and supergravity. Advances in Theoretical and Mathematical Physics, 2, 231-252102.Dimensionalidad y Machine LearningBellman, R. (1961). Adaptive control processes: A guided tour. Princeton University Press103.QuÃ­mica TermodinÃ¡micaFlowers, P., Theopold, K., Langley Richard, & Robinson, W.R. (2015). Chemistry. OpenStax104.
